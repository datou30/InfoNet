model:
  latent_dim: 256
  latent_num: 256
  input_dim: 2
  decoder_query_dim: 1000
  cross_attn_heads: 8
  self_attn_heads: 16
  num_self_attn_per_block: 8
  num_self_attn_blocks: 1

training:
  batchsize: 32
  seq_len: 5000
  learning_rate: 0.0002
  